{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a96841",
   "metadata": {},
   "source": [
    "### Importing necessary libraries and loading our data into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2627916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdb51b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rss(X, theta, y):\n",
    "    r = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        # taking the ith sample for measurement\n",
    "        new_sample = np.array([X[i]])\n",
    "        # adding a constant feature column as mentioned in lecture slides\n",
    "        new_sample_b = np.concatenate([np.ones((len(new_sample), 1)), new_sample], axis=1)\n",
    "        # predicting y-hat using our linear regression model\n",
    "        new_sample_pred = new_sample_b.dot(theta)\n",
    "        r += (new_sample_pred - y[i])**2\n",
    "    return r\n",
    "\n",
    "def tss(y):\n",
    "    t = 0\n",
    "    avg = np.mean(y)\n",
    "    for i in range(y.shape[0]):\n",
    "        t += (y[i] - avg)**2\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61266121",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('real-estate.csv', delimiter=',')\n",
    "X = np.delete(X, (0), axis=0) # Removing first column containing serial numbers\n",
    "X = np.delete(X, (0), axis=1) # Removing first row containing feature labels\n",
    "y = X[:, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07978405",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.delete(X, -1, axis=1)\n",
    "oneX = np.ones((len(X), 1))\n",
    "data = np.concatenate([oneX, X], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34039494",
   "metadata": {},
   "source": [
    "### Implementing the Normal Equation Approach to Linear Regression (derived below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5953c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtd = data.T.dot(data)\n",
    "theta = np.linalg.inv(data.T.dot(data)).dot(data.T).dot(y)\n",
    "# intercept, *coef = theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b0b9ef",
   "metadata": {},
   "source": [
    "### Calculating R2 (evaluation metric discussed in class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a75a39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58237045]\n"
     ]
    }
   ],
   "source": [
    "r2 = 1 - (rss(X, theta, y)/tss(y))\n",
    "print(r2)\n",
    "\n",
    "# r2 comes out to be 0.5824"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d084781d",
   "metadata": {},
   "source": [
    "# Derivation of the Normal Equation approach\n",
    "\n",
    "Here I will present a broad derivation of the Normal Equation approach in my own words.\n",
    "Let Q be a column vector of weights. The hypothesis equation for Linear Regression is:\n",
    "\n",
    "$$\n",
    "h(Q) = Q^TX\n",
    "$$\n",
    "\n",
    "To get the cost function in vector form, we first observe this quantity: (where n is the number of features)\n",
    "\n",
    "$$\n",
    "1/2m*(\n",
    "\\begin{bmatrix}\n",
    "Q^T(x^0) \\\\\n",
    "Q^T(x^0) \\\\\n",
    "... \\\\\n",
    "Q^T(x^n) \\\\\n",
    "\\end{bmatrix} - Y)\n",
    "$$\n",
    "\n",
    "As the cost function involves squares, we multiply it with its transpose:\n",
    "\n",
    "$$\n",
    "Cost(Q) = 1/2m*(XQ-Y)^T(XQ-Y) \\\\\n",
    "$$\n",
    "\n",
    "After some more simplification, we arrive at the following expression:\n",
    "\n",
    "$$\n",
    "Cost(Q) = 1/2m*((XQ)^TXQ - (XQ)^TY - Y^T(XQ) + Y^TY) \\\\\n",
    "or \\\\Cost(Q) = 1/2m*((XQ)^TXQ - 2*(XQ)^TY + Y^TY)\n",
    "$$\n",
    "\n",
    "Now, all we essentially need to do is to minimise this cost. As our cost function is a function in a single variable (Q), we can use basic linear algebra covered in our first semester to find its minima.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (Cost)}{\\partial Q} = 2X^TXQ - 2X^Ty\n",
    "$$\n",
    "\n",
    "Equating this to zero will give us our answer. Hence we proceed:\n",
    "\n",
    "$$\n",
    "2X^TXQ - 2X^Ty = 0 \\\\ or \\\\\n",
    "X^TXQ = X^Ty\n",
    "$$\n",
    "\n",
    "For the next step to be valid, the matrix $X^TX$ has to be invertible. Our final step of the derivation will be:\n",
    "\n",
    "$$\n",
    "(X^TX)^{-1}X^TXQ = (X^TX)^{-1}X^Ty \\\\ or \\\\\n",
    "Q = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "Which completes our derivation.\n",
    "\n",
    "## Limitations of the Normal Equation Approach\n",
    "\n",
    "The step to invert a matrix effectively runs in O(n^3) time. This can severely hurt our algorithm’s performance for large values of n. Therefore for large datasets, gradient descent is the preferred way to obtain linear regression coefficients.\n",
    "\n",
    "## References\n",
    "\n",
    "- **E. Bendersky**, “Derivation of the normal equation for linear regression,” *Eli Bendersky's Blog*. [Online]. Available: https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression. [Accessed: 01-Apr-2023]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9407e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
